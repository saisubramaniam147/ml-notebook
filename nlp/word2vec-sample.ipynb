{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Semantic Similarity using Word2Vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "- Word2Vec is a group of related models that are used to produce word embeddings.\n",
    "\n",
    "\n",
    "- Models are shallow neural networks that are trained to reconstruct linguistic contexts of words. \n",
    "\n",
    "\n",
    "- Word2Vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. It represents words as M-dimensional real vectors.\n",
    "\n",
    "\n",
    "- Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word2vec tends to indicate similar words – but the kind of similarity it learns includes more than just pure synonyms.\n",
    "\n",
    "- Word2Vec takes raw text as input and learns a word by\n",
    "    - Predicting its surrounding context (in the case of the skip-gram model), or,\n",
    "    - Predict a word given its surrounding context (in the case of the Continous Bag of Words, CBOW model),  using gradient descent with randomly initialized vectors.\n",
    "\n",
    "- The probability of predicting a word (Vo) given its 'context' (Vi) relates to maximising the joint distribution: log(P(Vi/Vo)/P(Vi))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process:\n",
    "\n",
    "- Represent words as 'one-hot' encoded vectors\n",
    "- Initialize weights based on the count of words and the required dimension (WxD)\n",
    "- Compute the vectors in the forward propogation phase and adjust the weights in the back-propagation phase.\n",
    "- Iterate above steps until minimum error or convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"skip_gram_net_arch.png\" width=\"100%\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<img src=\"skip_gram_net_arch.png\" width=\"100%\">')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive compositionality\n",
    "\n",
    "Word vectors obtained from Word2Vec show not only syntactic but also semantic relationships between words. They exhibit a property referred to as 'additive compositionality'. This means word vectors can be seen as representing the distribution of the context in which a word appears and the sum of vectors roughly represents an AND concatenation. This means algebraic operation can be performed on the word vectors.\n",
    "\n",
    "#### E.g. vector(‘King’) – vector(‘Man’) + vector(‘Woman’) = vector(‘Queen’)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Word     Score\n",
      "0  queen          0.931412\n",
      "1  monarch        0.858534\n",
      "2  princess       0.847657\n",
      "3  Queen_Consort  0.815027\n",
      "4  queens         0.809982\n"
     ]
    }
   ],
   "source": [
    "print pandas.DataFrame(model_wv_google.most_similar_cosmul(positive=['king', 'woman'], negative=['man'], topn=5), columns=['Word','Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Measures\n",
    "\n",
    "##### Cosine\n",
    "This method computes cosine similarity between a simple mean of the projection weight vectors of the given words and the vectors for each word in the model. The method corresponds to the word-analogy and distance scripts in the original word2vec implementation.\n",
    "\n",
    "##### CosMul\n",
    "Helps to find the top-N most similar words using the multiplicative combination objective. Positive words still contribute positively towards the similarity, negative words negatively, but with less susceptibility to one large distance dominating the calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Trained Word Embedding Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. GoogleNews\n",
    "\n",
    "#### Pre-trained Google News corpus (~100 billion words) word vector model (3 million 300-dimension English word vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-27 07:46:52,778 : INFO : loading projection weights from ../embedding/GoogleNews-vectors-negative300.bin.gz\n",
      "2017-10-27 07:50:28,154 : INFO : loaded (3000000, 300) matrix from ../embedding/GoogleNews-vectors-negative300.bin.gz\n",
      "2017-10-27 07:50:28,156 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Synonym for words....\n",
      "Words: happy, sorrow, country, food, magic, star, fintech, movies, animation, retail\n",
      "WORD: happy\n",
      "Words similar by Cosine similarity: \n",
      "           Word     Score\n",
      "0  glad          0.740889\n",
      "1  pleased       0.663217\n",
      "2  ecstatic      0.662691\n",
      "3  overjoyed     0.659929\n",
      "4  thrilled      0.651405\n",
      "5  satisfied     0.643795\n",
      "6  proud         0.636042\n",
      "7  delighted     0.627238\n",
      "8  disappointed  0.626995\n",
      "9  excited       0.624767\n",
      "Words similar by CosMul similarity: \n",
      "           Word     Score\n",
      "0  glad          0.870444\n",
      "1  pleased       0.831608\n",
      "2  ecstatic      0.831345\n",
      "3  overjoyed     0.829964\n",
      "4  thrilled      0.825702\n",
      "5  satisfied     0.821897\n",
      "6  proud         0.818020\n",
      "7  delighted     0.813618\n",
      "8  disappointed  0.813497\n",
      "9  excited       0.812383\n",
      "WORD: sorrow\n",
      "Words similar by Cosine similarity: \n",
      "               Word     Score\n",
      "0  sadness           0.865306\n",
      "1  anguish           0.763049\n",
      "2  grief             0.754967\n",
      "3  profound_sorrow   0.688777\n",
      "4  deepest_sorrow    0.687568\n",
      "5  heartfelt_sorrow  0.650902\n",
      "6  profound_sadness  0.648285\n",
      "7  despair           0.635771\n",
      "8  sorrowful         0.635302\n",
      "9  saddness          0.630314\n",
      "Words similar by CosMul similarity: \n",
      "               Word     Score\n",
      "0  sadness           0.932652\n",
      "1  anguish           0.881524\n",
      "2  grief             0.877483\n",
      "3  profound_sorrow   0.844388\n",
      "4  deepest_sorrow    0.843783\n",
      "5  heartfelt_sorrow  0.825450\n",
      "6  profound_sadness  0.824142\n",
      "7  despair           0.817885\n",
      "8  sorrowful         0.817650\n",
      "9  saddness          0.815156\n",
      "WORD: country\n",
      "Words similar by Cosine similarity: \n",
      "            Word     Score\n",
      "0  nation         0.724287\n",
      "1  continent      0.613089\n",
      "2  region         0.601515\n",
      "3  thecountry     0.600183\n",
      "4  world          0.598039\n",
      "5  coun_try       0.591695\n",
      "6  United_States  0.570611\n",
      "7  countrys       0.564198\n",
      "8  coutnry        0.542586\n",
      "9  counry         0.520168\n",
      "Words similar by CosMul similarity: \n",
      "            Word     Score\n",
      "0  nation         0.862143\n",
      "1  continent      0.806544\n",
      "2  region         0.800757\n",
      "3  thecountry     0.800091\n",
      "4  world          0.799019\n",
      "5  coun_try       0.795847\n",
      "6  United_States  0.785305\n",
      "7  countrys       0.782098\n",
      "8  coutnry        0.771292\n",
      "9  counry         0.760083\n",
      "WORD: food\n",
      "Words similar by Cosine similarity: \n",
      "               Word     Score\n",
      "0  foods             0.680492\n",
      "1  Food              0.653890\n",
      "2  foodstuffs        0.642583\n",
      "3  meals             0.616669\n",
      "4  food_stuffs       0.592864\n",
      "5  nourishing_meals  0.584761\n",
      "6  foodstuff         0.583522\n",
      "7  staple_foods      0.553579\n",
      "8  nutritious        0.546645\n",
      "9  meal              0.543371\n",
      "Words similar by CosMul similarity: \n",
      "               Word     Score\n",
      "0  foods             0.840245\n",
      "1  Food              0.826944\n",
      "2  foodstuffs        0.821291\n",
      "3  meals             0.808334\n",
      "4  food_stuffs       0.796431\n",
      "5  nourishing_meals  0.792380\n",
      "6  foodstuff         0.791761\n",
      "7  staple_foods      0.776789\n",
      "8  nutritious        0.773322\n",
      "9  meal              0.771685\n",
      "WORD: magic\n",
      "Words similar by Cosine similarity: \n",
      "               Word     Score\n",
      "0  magical           0.737332\n",
      "1  wizardry          0.554209\n",
      "2  enchantment       0.543434\n",
      "3  magician          0.542439\n",
      "4  pixie_dust        0.534964\n",
      "5  conjuring_tricks  0.526470\n",
      "6  Magical           0.518721\n",
      "7  magic_potion      0.512802\n",
      "8  brew_potions      0.505073\n",
      "9  alchemy           0.502909\n",
      "Words similar by CosMul similarity: \n",
      "               Word     Score\n",
      "0  magical           0.868665\n",
      "1  wizardry          0.777104\n",
      "2  enchantment       0.771716\n",
      "3  magician          0.771219\n",
      "4  pixie_dust        0.767481\n",
      "5  conjuring_tricks  0.763234\n",
      "6  Magical           0.759360\n",
      "7  magic_potion      0.756400\n",
      "8  brew_potions      0.752536\n",
      "9  alchemy           0.751454\n",
      "WORD: star\n",
      "Words similar by Cosine similarity: \n",
      "                    Word     Score\n",
      "0  stars                  0.776396\n",
      "1  superstar              0.734060\n",
      "2  starlet                0.638106\n",
      "3  megastar               0.616512\n",
      "4  heart_throb            0.572670\n",
      "5  teen_heart_throb       0.550373\n",
      "6  heart_throb_Zac_Efron  0.544349\n",
      "7  heartthrob             0.543801\n",
      "8  superstars             0.532613\n",
      "9  standout               0.521036\n",
      "Words similar by CosMul similarity: \n",
      "                    Word     Score\n",
      "0  stars                  0.888197\n",
      "1  superstar              0.867029\n",
      "2  starlet                0.819052\n",
      "3  megastar               0.808255\n",
      "4  heart_throb            0.786334\n",
      "5  teen_heart_throb       0.775186\n",
      "6  heart_throb_Zac_Efron  0.772174\n",
      "7  heartthrob             0.771900\n",
      "8  superstars             0.766306\n",
      "9  standout               0.760517\n",
      "WORD: movies\n",
      "Words similar by Cosine similarity: \n",
      "                     Word     Score\n",
      "0  films                   0.859124\n",
      "1  movie                   0.801311\n",
      "2  flicks                  0.698074\n",
      "3  film                    0.685713\n",
      "4  Hollywood_blockbusters  0.676363\n",
      "5  Movies                  0.670834\n",
      "6  horror_flicks           0.648432\n",
      "7  blockbusters            0.645472\n",
      "8  indie_flicks            0.633691\n",
      "9  romcoms                 0.618649\n",
      "Words similar by CosMul similarity: \n",
      "                     Word     Score\n",
      "0  films                   0.929561\n",
      "1  movie                   0.900655\n",
      "2  flicks                  0.849036\n",
      "3  film                    0.842856\n",
      "4  Hollywood_blockbusters  0.838181\n",
      "5  Movies                  0.835416\n",
      "6  horror_flicks           0.824215\n",
      "7  blockbusters            0.822735\n",
      "8  indie_flicks            0.816845\n",
      "9  romcoms                 0.809324\n",
      "WORD: animation\n",
      "Words similar by Cosine similarity: \n",
      "             Word     Score\n",
      "0  3D_animation    0.739124\n",
      "1  animators       0.728812\n",
      "2  Animation       0.721526\n",
      "3  CG_animation    0.700052\n",
      "4  2D_animation    0.696659\n",
      "5  animated        0.676545\n",
      "6  animator        0.666437\n",
      "7  CGI_animation   0.661282\n",
      "8  visual_effects  0.656114\n",
      "9  animations      0.643687\n",
      "Words similar by CosMul similarity: \n",
      "             Word     Score\n",
      "0  3D_animation    0.869561\n",
      "1  animators       0.864405\n",
      "2  Animation       0.860762\n",
      "3  CG_animation    0.850025\n",
      "4  2D_animation    0.848329\n",
      "5  animated        0.838272\n",
      "6  animator        0.833218\n",
      "7  CGI_animation   0.830640\n",
      "8  visual_effects  0.828056\n",
      "9  animations      0.821842\n",
      "WORD: retail\n",
      "Words similar by Cosine similarity: \n",
      "                             Word     Score\n",
      "0  retailing                       0.765857\n",
      "1  Retail                          0.725034\n",
      "2  retailers                       0.638055\n",
      "3  professionals_NRF_SmartBrief    0.629336\n",
      "4  retail_outlets                  0.620026\n",
      "5  stores                          0.585080\n",
      "6  sales                           0.572954\n",
      "7  www.southbendtribune.com_blogs  0.567318\n",
      "8  nonretail                       0.564543\n",
      "9  Retails                         0.562881\n",
      "Words similar by CosMul similarity: \n",
      "                             Word     Score\n",
      "0  retailing                       0.882927\n",
      "1  Retail                          0.862516\n",
      "2  retailers                       0.819027\n",
      "3  professionals_NRF_SmartBrief    0.814667\n",
      "4  retail_outlets                  0.810012\n",
      "5  stores                          0.792539\n",
      "6  sales                           0.786476\n",
      "7  www.southbendtribune.com_blogs  0.783658\n",
      "8  nonretail                       0.782271\n",
      "9  Retails                         0.781440\n",
      "WORD: fintech\n",
      "Words similar by Cosine similarity: \n",
      "                           Word     Score\n",
      "0  Oracle_NasdaqGS_ORCL          0.549123\n",
      "1  ColorPalette_OMS              0.548577\n",
      "2  Ken_Dummitt_president         0.548203\n",
      "3  Q_Nami                        0.546736\n",
      "4  scalable_analytic             0.543391\n",
      "5  M_&A;_finance                 0.540588\n",
      "6  SWIFTNet_connectivity         0.536495\n",
      "7  BNP_Paribas_Société_Générale  0.533299\n",
      "8  Alnova                        0.532971\n",
      "9  Boomi_AtomSphere              0.532332\n",
      "Words similar by CosMul similarity: \n",
      "                           Word     Score\n",
      "0  Oracle_NasdaqGS_ORCL          0.774561\n",
      "1  ColorPalette_OMS              0.774288\n",
      "2  Ken_Dummitt_president         0.774101\n",
      "3  Q_Nami                        0.773367\n",
      "4  scalable_analytic             0.771695\n",
      "5  M_&A;_finance                 0.770293\n",
      "6  SWIFTNet_connectivity         0.768247\n",
      "7  BNP_Paribas_Société_Générale  0.766649\n",
      "8  Alnova                        0.766485\n",
      "9  Boomi_AtomSphere              0.766165\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas\n",
    "from gensim import models\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# load word vectors\n",
    "model_wv_google = models.KeyedVectors.load_word2vec_format('../embedding/GoogleNews-vectors-negative300.bin.gz',binary=True)\n",
    "\n",
    "print(\"Computing Synonym for words....\")\n",
    "print(\"Words: happy, sorrow, country, food, magic, star, fintech, movies, animation, retail\")\n",
    "\n",
    "# define array of words\n",
    "words_array = ['happy', 'sorrow', 'country', 'food', 'magic', 'star', 'movies', 'animation', 'retail', 'fintech']\n",
    "\n",
    "for word in words_array:\n",
    "    print(\"WORD: \" + word)\n",
    "    try:\n",
    "        print(\"Words similar by Cosine similarity: \")\n",
    "        df = pandas.DataFrame(model_wv_google.most_similar(positive=[word], topn=10), columns=['Word','Score'])\n",
    "        print(df)\n",
    "        print(\"Words similar by CosMul similarity: \")\n",
    "        df = pandas.DataFrame(model_wv_google.most_similar_cosmul(positive=[word], topn=10), columns=['Word','Score'])\n",
    "        print(df)\n",
    "    except KeyError:\n",
    "        print(\"Word not found - \" + word + \". Skipping...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. GloVe: Global Vectors for Word Representation\n",
    "#### Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download): glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-27 07:52:25,408 : INFO : loading projection weights from ../embedding/glove-word2vec.txt\n",
      "2017-10-27 07:54:40,122 : INFO : loaded (400000, 300) matrix from ../embedding/glove-word2vec.txt\n",
      "2017-10-27 07:54:40,124 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Synonymn for words....\n",
      "Words: happy, sorrow, country, food, magic, star, fintech, movies, animation, retail\n",
      "WORD: happy\n",
      "Words similar by Cosine similarity: \n",
      "           0         1\n",
      "0  'm         0.708012\n",
      "1  glad       0.690503\n",
      "2  pleased    0.671247\n",
      "3  really     0.657590\n",
      "4  always     0.649467\n",
      "5  everyone   0.644904\n",
      "6  everybody  0.634436\n",
      "7  feel       0.633681\n",
      "8  i          0.629831\n",
      "9  sure       0.629091\n",
      "Words similar by CosMul similarity: \n",
      "           0         1\n",
      "0  'm         0.854005\n",
      "1  glad       0.845251\n",
      "2  pleased    0.835623\n",
      "3  really     0.828794\n",
      "4  always     0.824733\n",
      "5  everyone   0.822451\n",
      "6  everybody  0.817217\n",
      "7  feel       0.816840\n",
      "8  i          0.814915\n",
      "9  sure       0.814544\n",
      "WORD: sorrow\n",
      "Words similar by Cosine similarity: \n",
      "             0         1\n",
      "0  sadness      0.835528\n",
      "1  grief        0.756200\n",
      "2  anguish      0.738144\n",
      "3  regret       0.705060\n",
      "4  gratitude    0.632629\n",
      "5  condolences  0.618488\n",
      "6  despair      0.614616\n",
      "7  sympathy     0.599212\n",
      "8  remorse      0.591213\n",
      "9  heartfelt    0.590358\n",
      "Words similar by CosMul similarity: \n",
      "             0         1\n",
      "0  sadness      0.917763\n",
      "1  grief        0.878099\n",
      "2  anguish      0.869071\n",
      "3  regret       0.852529\n",
      "4  gratitude    0.816314\n",
      "5  condolences  0.809243\n",
      "6  despair      0.807307\n",
      "7  sympathy     0.799605\n",
      "8  remorse      0.795606\n",
      "9  heartfelt    0.795178\n",
      "WORD: country\n",
      "Words similar by Cosine similarity: \n",
      "           0         1\n",
      "0  nation     0.768070\n",
      "1  countries  0.625222\n",
      "2  already    0.561694\n",
      "3  has        0.560651\n",
      "4  where      0.544559\n",
      "5  now        0.541266\n",
      "6  since      0.541141\n",
      "7  all        0.541099\n",
      "8  once       0.540835\n",
      "9  continent  0.540486\n",
      "Words similar by CosMul similarity: \n",
      "           0         1\n",
      "0  nation     0.884034\n",
      "1  countries  0.812610\n",
      "2  already    0.780846\n",
      "3  has        0.780325\n",
      "4  where      0.772279\n",
      "5  now        0.770632\n",
      "6  since      0.770570\n",
      "7  all        0.770549\n",
      "8  once       0.770417\n",
      "9  continent  0.770242\n",
      "WORD: food\n",
      "Words similar by Cosine similarity: \n",
      "            0         1\n",
      "0  foods       0.669407\n",
      "1  supplies    0.606881\n",
      "2  eat         0.590300\n",
      "3  meat        0.567760\n",
      "4  eating      0.567423\n",
      "5  meals       0.567192\n",
      "6  products    0.549430\n",
      "7  meal        0.546472\n",
      "8  vegetables  0.544741\n",
      "9  nutrition   0.538570\n",
      "Words similar by CosMul similarity: \n",
      "            0         1\n",
      "0  foods       0.834703\n",
      "1  supplies    0.803440\n",
      "2  eat         0.795149\n",
      "3  meat        0.783879\n",
      "4  eating      0.783711\n",
      "5  meals       0.783595\n",
      "6  products    0.774715\n",
      "7  meal        0.773235\n",
      "8  vegetables  0.772370\n",
      "9  nutrition   0.769284\n",
      "WORD: magic\n",
      "Words similar by Cosine similarity: \n",
      "          0         1\n",
      "0  magical   0.694497\n",
      "1  wizards   0.524983\n",
      "2  wand      0.487364\n",
      "3  orlando   0.473295\n",
      "4  magician  0.466960\n",
      "5  lakers    0.461160\n",
      "6  tricks    0.453158\n",
      "7  potion    0.448855\n",
      "8  trick     0.445874\n",
      "9  celtics   0.443246\n",
      "Words similar by CosMul similarity: \n",
      "          0         1\n",
      "0  magical   0.847248\n",
      "1  wizards   0.762491\n",
      "2  wand      0.743682\n",
      "3  orlando   0.736647\n",
      "4  magician  0.733479\n",
      "5  lakers    0.730579\n",
      "6  tricks    0.726578\n",
      "7  potion    0.724427\n",
      "8  trick     0.722936\n",
      "9  celtics   0.721623\n",
      "WORD: star\n",
      "Words similar by Cosine similarity: \n",
      "           0         1\n",
      "0  stars      0.791285\n",
      "1  superstar  0.593605\n",
      "2  actor      0.506907\n",
      "3  movie      0.504511\n",
      "4  player     0.481758\n",
      "5  actress    0.475845\n",
      "6  starred    0.472311\n",
      "7  hollywood  0.469557\n",
      "8  veteran    0.467706\n",
      "9  starring   0.467390\n",
      "Words similar by CosMul similarity: \n",
      "           0         1\n",
      "0  stars      0.895642\n",
      "1  superstar  0.796801\n",
      "2  actor      0.753453\n",
      "3  movie      0.752255\n",
      "4  player     0.740878\n",
      "5  actress    0.737922\n",
      "6  starred    0.736155\n",
      "7  hollywood  0.734778\n",
      "8  veteran    0.733852\n",
      "9  starring   0.733695\n",
      "WORD: fintech\n",
      "Words similar by Cosine similarity: \n",
      "Word not found - fintech. Skipping...\n",
      "WORD: movies\n",
      "Words similar by Cosine similarity: \n",
      "               0         1\n",
      "0  films          0.873983\n",
      "1  movie          0.849347\n",
      "2  film           0.716739\n",
      "3  videos         0.614025\n",
      "4  hollywood      0.611163\n",
      "5  documentaries  0.599998\n",
      "6  comedy         0.596902\n",
      "7  comedies       0.593223\n",
      "8  horror         0.589994\n",
      "9  cinema         0.585605\n",
      "Words similar by CosMul similarity: \n",
      "               0         1\n",
      "0  films          0.936991\n",
      "1  movie          0.924673\n",
      "2  film           0.858369\n",
      "3  videos         0.807012\n",
      "4  hollywood      0.805581\n",
      "5  documentaries  0.799998\n",
      "6  comedy         0.798450\n",
      "7  comedies       0.796611\n",
      "8  horror         0.794996\n",
      "9  cinema         0.792801\n",
      "WORD: animation\n",
      "Words similar by Cosine similarity: \n",
      "             0         1\n",
      "0  animated     0.711796\n",
      "1  animators    0.613930\n",
      "2  pixar        0.611175\n",
      "3  animations   0.591563\n",
      "4  stop-motion  0.577952\n",
      "5  animator     0.575989\n",
      "6  cartoon      0.573907\n",
      "7  live-action  0.572038\n",
      "8  dreamworks   0.553086\n",
      "9  studios      0.537903\n",
      "Words similar by CosMul similarity: \n",
      "             0         1\n",
      "0  animated     0.855897\n",
      "1  animators    0.806964\n",
      "2  pixar        0.805587\n",
      "3  animations   0.795780\n",
      "4  stop-motion  0.788975\n",
      "5  animator     0.787994\n",
      "6  cartoon      0.786953\n",
      "7  live-action  0.786018\n",
      "8  dreamworks   0.776542\n",
      "9  studios      0.768951\n",
      "WORD: retail\n",
      "Words similar by Cosine similarity: \n",
      "            0         1\n",
      "0  retailers   0.702454\n",
      "1  retailing   0.698367\n",
      "2  stores      0.674252\n",
      "3  sales       0.651369\n",
      "4  wholesale   0.635678\n",
      "5  store       0.599881\n",
      "6  consumer    0.597639\n",
      "7  retailer    0.575743\n",
      "8  shopping    0.572694\n",
      "9  businesses  0.571442\n",
      "Words similar by CosMul similarity: \n",
      "            0         1\n",
      "0  retailers   0.851226\n",
      "1  retailing   0.849182\n",
      "2  stores      0.837125\n",
      "3  sales       0.825684\n",
      "4  wholesale   0.817838\n",
      "5  store       0.799940\n",
      "6  consumer    0.798819\n",
      "7  retailer    0.787871\n",
      "8  shopping    0.786346\n",
      "9  businesses  0.785720\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas\n",
    "from gensim import models\n",
    "import pandas\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# load word vectors\n",
    "model_wv_glove = models.KeyedVectors.load_word2vec_format('../embedding/glove-word2vec.txt')\n",
    "\n",
    "print(\"Computing Synonymn for words....\")\n",
    "print(\"Words: happy, sorrow, country, food, magic, star, fintech, movies, animation, retail\")\n",
    "\n",
    "# define array of words\n",
    "words_array = ['happy', 'sorrow', 'country', 'food', 'magic', 'star', 'fintech', 'movies', 'animation', 'retail']\n",
    "\n",
    "for word in words_array:\n",
    "    print(\"WORD: \" + word)\n",
    "    try:\n",
    "        print(\"Words similar by Cosine similarity: \")\n",
    "        df = pandas.DataFrame(model_wv_glove.most_similar(positive=[word], topn=10))\n",
    "        print df\n",
    "        print(\"Words similar by CosMul similarity: \")\n",
    "        df = pandas.DataFrame(model_wv_glove.most_similar_cosmul(positive=[word], topn=10))\n",
    "        print df\n",
    "    except KeyError:\n",
    "        print(\"Word not found - \" + word + \". Skipping...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model with custom dataset\n",
    "We can also train our own Word2vec model using a custom dataset, to extract domain relevant synonym and similarities.\n",
    "\n",
    "#### Broadly three phases:\n",
    "- Text pre-processing and feature generation\n",
    "- Training the model providing the features and training parameters\n",
    "- Save the model and load for future use\n",
    "\n",
    "\n",
    "#### The pre-processing process is as follows:\n",
    "- Tokenize input sentences/paragrahs and convert each word token into lowercase\n",
    "- Remove stopwords (e.g. 'and','the','a')\n",
    "- Perform Lemmatization so as to bring word tokens to the base format.\n",
    "  Lemmatisation is used to determine the lemma of a word based on its intended meaning. Unlike stemming, lemmatisation depends on the intended part of speech and meaning of a word in a sentence.\n",
    "\n",
    "#### While training the model, we consider the following parameters:\n",
    "- Input sentences (obtained from previous step)\n",
    "- Size of the word vector dimension (300)\n",
    "- Window size to consider as the maximum distance between the current and predicted word within a sentence\n",
    "- Minimum frequency a word should occur to be considered for training (3)\n",
    "- Number of iterations (epoch)\n",
    "- Mode chosen - either Continous Bag Of Words(CBOW) or Skip-gram (we choose Skip-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from gensim import models\n",
    "import sys\n",
    "import os.path\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Read training file and load to array\n",
    "def readFileAndPreprocess(filename):\n",
    "\tstopwords = nltk.corpus.stopwords.words('english') + list(string.punctuation)\n",
    "\twordnet_lemmatizer = WordNetLemmatizer()\n",
    "\tsentences = [] # init empty array for training data\n",
    "\twith open(filename) as f:\n",
    "\t\tcontent = f.readlines()\n",
    "\t\t# For each record remove stopwords\n",
    "\t\tfor record in content:\n",
    "\t\t\trecord = unicode(record.lower(), \"utf-8\")\n",
    "\t\t\trecord = word_tokenize(record)\n",
    "\t\t\trecord = [wordnet_lemmatizer.lemmatize(x) for x in record if x not in string.punctuation ]\n",
    "\t\t\tsentence = [w for w in record if w not in stopwords]\n",
    "\t\t\tsentences.append(sentence)\n",
    "\treturn sentences\n",
    "\n",
    "training_set = sys.argv[1]\n",
    "\n",
    "try:\n",
    "\tos.path.isfile(training_set) # check if file exists\n",
    "\tprint(\"Reading training data from file: \" + training_set)\n",
    "\tsentences = readFileAndPreprocess(training_set)\n",
    "\n",
    "\t# Train word2vec model and save word vectors to file\n",
    "\tprint(\"Training Word2Vec model....\")\n",
    "\tmodel = models.Word2Vec(sentences, size=300, window=5, min_count=3, workers=64, iter=1000, sg=1) #skip-gram,sg=1\n",
    "\tword2vec_model = model\n",
    "\tprint(\"Model Completed training!\")\n",
    "\tprint(\"Saving model..\")\n",
    "\tword2vec_model.save(sys.argv[2])\n",
    "\tprint(\"Model stored in: \" + sys.argv[2])\n",
    "\n",
    "except OSError:\n",
    "\tprint(\"Invalid File path!. Exiting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Amazon Food Dining Dataset\n",
    "#### ~500,000 food reviews from Amazon https://www.kaggle.com/snap/amazon-fine-food-reviews/data \n",
    "\n",
    "The Amazon Fine Food Reviews dataset consists of 568,454 food reviews Amazon users left up to October 2012.\n",
    "\n",
    " Trained on 2532725200 raw words (2254738977 effective words).\n",
    " \n",
    " Iterations: 100, Dimension size: 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-27 07:56:52,993 : INFO : loading Word2Vec object from ../embedding/food-word2vec.model\n",
      "2017-10-27 07:56:53,283 : INFO : loading wv recursively from ../embedding/food-word2vec.model.wv.* with mmap=None\n",
      "2017-10-27 07:56:53,285 : INFO : loading syn0 from ../embedding/food-word2vec.model.wv.syn0.npy with mmap=None\n",
      "2017-10-27 07:56:54,225 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-10-27 07:56:54,226 : INFO : loading syn1neg from ../embedding/food-word2vec.model.syn1neg.npy with mmap=None\n",
      "2017-10-27 07:56:55,177 : INFO : setting ignored attribute cum_table to None\n",
      "2017-10-27 07:56:55,179 : INFO : loaded ../embedding/food-word2vec.model\n",
      "2017-10-27 07:56:55,352 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar by Cosine similarity: \n",
      "WORD: gift\n",
      "        Word     Score\n",
      "0  christmas  0.710675\n",
      "1  basket     0.663248\n",
      "2  recipient  0.648269\n",
      "3  birthday   0.624336\n",
      "4  present    0.551398\n",
      "5  stuffer    0.550811\n",
      "6  valentine  0.535837\n",
      "7  xmas       0.535195\n",
      "8  basket..   0.516378\n",
      "9  holiday    0.498228\n",
      "WORD: star\n",
      "        Word     Score\n",
      "0  rating     0.696299\n",
      "1  -minus     0.587418\n",
      "2  'leakers   0.553210\n",
      "3  stars.     0.526907\n",
      "4  half-star  0.512984\n",
      "5  assigning  0.499092\n",
      "6  dout       0.497873\n",
      "7  5          0.492000\n",
      "8  stars-     0.484024\n",
      "9  6/5        0.483482\n",
      "WORD: delivery\n",
      "        Word     Score\n",
      "0  shipment   0.608006\n",
      "1  delivered  0.598283\n",
      "2  shipping   0.596473\n",
      "3  godsend-   0.542538\n",
      "4  shipped    0.539969\n",
      "5  service    0.537631\n",
      "6  prompt     0.529447\n",
      "7  ship       0.509131\n",
      "8  delivery.  0.491290\n",
      "9  updated..  0.482031\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas\n",
    "from gensim import models\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# load word vectors\n",
    "model = models.Word2Vec.load('../embedding/food-word2vec.model')\n",
    "model_wv_food = model.wv\n",
    "print(\"Words similar by Cosine similarity: \")\n",
    "words_array = ['gift', 'star', 'delivery']\n",
    "\n",
    "for word in words_array:\n",
    "    print(\"WORD: \" + word)\n",
    "    try:\n",
    "        df = pandas.DataFrame(model_wv_food.most_similar(positive=[word], topn=10), columns=['Word','Score'])\n",
    "        print(df)\n",
    "    except KeyError:\n",
    "        print(\"Word not found - \" + word + \". Skipping...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.A Bible in basic English\n",
    "#### Kaggle dataset https://www.kaggle.com/oswinrh/bible/data \n",
    "Trained on 407239000 raw words (295296026 effective words).\n",
    "\n",
    "Iterations: 1000, Dimension size: 300\n",
    "\n",
    "Using Skip-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-27 07:57:05,961 : INFO : loading Word2Vec object from ../embedding/bible-word2vec.model\n",
      "2017-10-27 07:57:06,163 : INFO : loading wv recursively from ../embedding/bible-word2vec.model.wv.* with mmap=None\n",
      "2017-10-27 07:57:06,164 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-10-27 07:57:06,165 : INFO : setting ignored attribute cum_table to None\n",
      "2017-10-27 07:57:06,166 : INFO : loaded ../embedding/bible-word2vec.model\n",
      "2017-10-27 07:57:06,174 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar by Cosine similarity: \n",
      "WORD: god\n",
      "               Word     Score\n",
      "0  lord              0.468206\n",
      "1  give              0.321597\n",
      "2  grace             0.306305\n",
      "3  ithrite           0.295166\n",
      "4  mikloth           0.289910\n",
      "5  worshipper        0.285231\n",
      "6  ahio              0.284701\n",
      "7  wind-instruments  0.283968\n",
      "8  glory             0.283407\n",
      "9  worship           0.282527\n",
      "WORD: star\n",
      "         Word     Score\n",
      "0  heaven      0.342713\n",
      "1  moon        0.288913\n",
      "2  two-edged   0.284936\n",
      "3  brighter    0.280657\n",
      "4  becoming    0.254215\n",
      "5  worshipper  0.251062\n",
      "6  bright      0.246979\n",
      "7  abraham     0.245216\n",
      "8  sun         0.242776\n",
      "9  unchanging  0.238530\n",
      "WORD: faith\n",
      "            Word     Score\n",
      "0  hope           0.333375\n",
      "1  christ         0.294178\n",
      "2  wisdom         0.276601\n",
      "3  salvation      0.276555\n",
      "4  love           0.274371\n",
      "5  god            0.270297\n",
      "6  hearer         0.267657\n",
      "7  based          0.264440\n",
      "8  timothy        0.262907\n",
      "9  righteousness  0.260266\n",
      "WORD: cross\n",
      "        Word     Score\n",
      "0  jesus      0.304253\n",
      "1  nazarene   0.289648\n",
      "2  abner      0.260132\n",
      "3  nailed     0.255574\n",
      "4  violently  0.252954\n",
      "5  attacked   0.252766\n",
      "6  shaming    0.246771\n",
      "7  police     0.245356\n",
      "8  christ     0.240331\n",
      "9  cruelly    0.238882\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas\n",
    "from gensim import models\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# load word vectors\n",
    "model = models.Word2Vec.load('../embedding/bible-word2vec.model')\n",
    "model_wv_bible_sg = model.wv\n",
    "\n",
    "# define array of words\n",
    "words_array = ['god','star','faith', 'cross']\n",
    "print(\"Words similar by Cosine similarity: \")\n",
    "    \n",
    "for word in words_array:\n",
    "    print(\"WORD: \" + word)\n",
    "    try:\n",
    "        df = pandas.DataFrame(model_wv_bible_sg.most_similar(positive=[word], topn=10), columns=['Word','Score'])\n",
    "        print(df)\n",
    "    except KeyError:\n",
    "        print(\"Word not found - \" + word + \". Skipping...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.B Bible\n",
    "Using Continous Bag of Words (CBOW) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-27 07:57:10,155 : INFO : loading Word2Vec object from ../embedding/bible-cbow-word2vec.model\n",
      "2017-10-27 07:57:10,301 : INFO : loading wv recursively from ../embedding/bible-cbow-word2vec.model.wv.* with mmap=None\n",
      "2017-10-27 07:57:10,302 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-10-27 07:57:10,303 : INFO : setting ignored attribute cum_table to None\n",
      "2017-10-27 07:57:10,304 : INFO : loaded ../embedding/bible-cbow-word2vec.model\n",
      "2017-10-27 07:57:10,313 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar by Cosine similarity: \n",
      "WORD: god\n",
      "    Word     Score\n",
      "0  lord   0.658754\n",
      "1  ''     0.543459\n",
      "2  ``     0.485857\n",
      "3  ha     0.393050\n",
      "4  say    0.383421\n",
      "5  u      0.375947\n",
      "6  give   0.375467\n",
      "7  name   0.367225\n",
      "8  given  0.363422\n",
      "9  ever   0.356873\n",
      "WORD: star\n",
      "     Word     Score\n",
      "0  sky     0.317151\n",
      "1  arch    0.316245\n",
      "2  snow    0.311528\n",
      "3  dew     0.297181\n",
      "4  black   0.295145\n",
      "5  mist    0.281507\n",
      "6  bright  0.269884\n",
      "7  sun     0.267162\n",
      "8  cloud   0.254671\n",
      "9  light   0.253995\n",
      "WORD: faith\n",
      "            Word     Score\n",
      "0  hope           0.427250\n",
      "1  christ         0.360321\n",
      "2  god            0.353349\n",
      "3  righteousness  0.349535\n",
      "4  true           0.342959\n",
      "5  law            0.342008\n",
      "6  word           0.317370\n",
      "7  wisdom         0.313897\n",
      "8  love           0.313820\n",
      "9  thing          0.307310\n",
      "WORD: cross\n",
      "        Word     Score\n",
      "0  nailed     0.330490\n",
      "1  pilate     0.324577\n",
      "2  barabbas   0.307545\n",
      "3  pointing   0.296651\n",
      "4  nazarene   0.291251\n",
      "5  underwent  0.284718\n",
      "6  undergo    0.259190\n",
      "7  pontius    0.258504\n",
      "8  jesus      0.257624\n",
      "9  didymus    0.255940\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas\n",
    "from gensim import models\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# load word vectors\n",
    "model = models.Word2Vec.load('../embedding/bible-cbow-word2vec.model')\n",
    "model_wv_bible_cbow = model.wv\n",
    "\n",
    "# define array of words\n",
    "words_array = ['god','star','faith', 'cross']\n",
    "print(\"Words similar by Cosine similarity: \")\n",
    "    \n",
    "for word in words_array:\n",
    "    print(\"WORD: \" + word)\n",
    "    try:\n",
    "        df = pandas.DataFrame(model_wv_bible_cbow.most_similar(positive=[word], topn=10), columns=['Word','Score'])\n",
    "        print(df)\n",
    "    except KeyError:\n",
    "        print(\"Word not found - \" + word + \". Skipping...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Similarities between models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar(word):\n",
    "    print(\"----------------******START*****----------------\")\n",
    "    print(\"WORD: \" + word)\n",
    "    print(\"Computing Similarity Using Model: GoogleNews\")\n",
    "    try:\n",
    "        df = pandas.DataFrame(model_wv_google.most_similar(positive=[word], topn=10), columns=['Word','Score'])\n",
    "        print(df)\n",
    "    except KeyError:\n",
    "        print(\"Word not found in GoogleNews - \" + word + \". Skipping...\")\n",
    "\n",
    "    print(\"Computing Similarity Using Model: GloVe\")\n",
    "    try:\n",
    "        df = pandas.DataFrame(model_wv_glove.most_similar(positive=[word], topn=10), columns=['Word','Score'])\n",
    "        print(df)\n",
    "    except KeyError:\n",
    "        print(\"Word not found in GloVe - \" + word + \". Skipping...\")\n",
    "\n",
    "    print(\"Computing Similarity Using Model: Food\")\n",
    "    try:\n",
    "        df = pandas.DataFrame(model_wv_food.most_similar(positive=[word], topn=10), columns=['Word','Score'])\n",
    "        print(df)\n",
    "    except KeyError:\n",
    "        print(\"Word not found in Food - \" + word + \". Skipping...\")\n",
    "        \n",
    "    print(\"Computing Similarity Using Model: Bible\")\n",
    "    try:\n",
    "        df = pandas.DataFrame(model_wv_bible_sg.most_similar(positive=[word], topn=10), columns=['Word','Score'])\n",
    "        print(df)\n",
    "    except KeyError:\n",
    "        print(\"Word not found in Bible - \" + word + \". Skipping...\")\n",
    "        \n",
    "    print(\"----------------******END*****----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try your own "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------******START*****----------------\n",
      "WORD: growth\n",
      "Computing Similarity Using Model: GoogleNews\n",
      "                   Word     Score\n",
      "0  growthin              0.622422\n",
      "1  grwoth                0.608003\n",
      "2  Growth                0.605946\n",
      "3  exponential_growth    0.576093\n",
      "4  expansion             0.573094\n",
      "5  reacceleration        0.564692\n",
      "6  double_digit_growths  0.561573\n",
      "7  noninflationary       0.559182\n",
      "8  economicgrowth        0.555412\n",
      "9  slowdown              0.553737\n",
      "Computing Similarity Using Model: GloVe\n",
      "       Word     Score\n",
      "0  economic  0.668446\n",
      "1  economy   0.667751\n",
      "2  slowing   0.640928\n",
      "3  increase  0.629197\n",
      "4  gdp       0.617421\n",
      "5  decline   0.611499\n",
      "6  rise      0.607873\n",
      "7  recovery  0.607726\n",
      "8  slowdown  0.606292\n",
      "9  grow      0.604086\n",
      "Computing Similarity Using Model: Food\n",
      "            Word     Score\n",
      "0  grow           0.470401\n",
      "1  growing        0.466358\n",
      "2  growth/fast    0.456966\n",
      "3  184.1555       0.445261\n",
      "4  streptococcus  0.441660\n",
      "5  pneumoniae     0.435152\n",
      "6  plant          0.428691\n",
      "7  bonsai         0.428630\n",
      "8  soil           0.421603\n",
      "9  pothos         0.416853\n",
      "Computing Similarity Using Model: Bible\n",
      "              Word     Score\n",
      "0  fruit            0.293162\n",
      "1  leaf             0.274625\n",
      "2  flowering        0.269790\n",
      "3  grass            0.262909\n",
      "4  olive-tree       0.260959\n",
      "5  camel-trains     0.258351\n",
      "6  banded           0.257236\n",
      "7  wide-stretching  0.254655\n",
      "8  learning         0.249600\n",
      "9  poisoned         0.247419\n",
      "----------------******END*****----------------\n",
      "----------------******START*****----------------\n",
      "WORD: star\n",
      "Computing Similarity Using Model: GoogleNews\n",
      "                    Word     Score\n",
      "0  stars                  0.776396\n",
      "1  superstar              0.734060\n",
      "2  starlet                0.638106\n",
      "3  megastar               0.616512\n",
      "4  heart_throb            0.572670\n",
      "5  teen_heart_throb       0.550373\n",
      "6  heart_throb_Zac_Efron  0.544349\n",
      "7  heartthrob             0.543801\n",
      "8  superstars             0.532613\n",
      "9  standout               0.521036\n",
      "Computing Similarity Using Model: GloVe\n",
      "        Word     Score\n",
      "0  stars      0.791285\n",
      "1  superstar  0.593605\n",
      "2  actor      0.506907\n",
      "3  movie      0.504511\n",
      "4  player     0.481758\n",
      "5  actress    0.475845\n",
      "6  starred    0.472311\n",
      "7  hollywood  0.469557\n",
      "8  veteran    0.467706\n",
      "9  starring   0.467390\n",
      "Computing Similarity Using Model: Food\n",
      "        Word     Score\n",
      "0  rating     0.696299\n",
      "1  -minus     0.587418\n",
      "2  'leakers   0.553210\n",
      "3  stars.     0.526907\n",
      "4  half-star  0.512984\n",
      "5  assigning  0.499092\n",
      "6  dout       0.497873\n",
      "7  5          0.492000\n",
      "8  stars-     0.484024\n",
      "9  6/5        0.483482\n",
      "Computing Similarity Using Model: Bible\n",
      "         Word     Score\n",
      "0  heaven      0.342713\n",
      "1  moon        0.288913\n",
      "2  two-edged   0.284936\n",
      "3  brighter    0.280657\n",
      "4  becoming    0.254215\n",
      "5  worshipper  0.251062\n",
      "6  bright      0.246979\n",
      "7  abraham     0.245216\n",
      "8  sun         0.242776\n",
      "9  unchanging  0.238530\n",
      "----------------******END*****----------------\n",
      "----------------******START*****----------------\n",
      "WORD: finance\n",
      "Computing Similarity Using Model: GoogleNews\n",
      "                            Word     Score\n",
      "0  Finance                        0.654987\n",
      "1  financing                      0.586815\n",
      "2  fi_nance                       0.550237\n",
      "3  reporter_Sue_Lannin            0.547500\n",
      "4  Ellen_Roseman_writes           0.547002\n",
      "5  professionals_ELFA_SmartBrief  0.543920\n",
      "6  director_Karim_Naffah          0.538370\n",
      "7  initiatives_PFIs               0.536559\n",
      "8  Agha_Jan_Mohtasim              0.530542\n",
      "9  fin_ance                       0.528773\n",
      "Computing Similarity Using Model: GloVe\n",
      "         Word     Score\n",
      "0  minister    0.576089\n",
      "1  financial   0.564457\n",
      "2  financing   0.551017\n",
      "3  banking     0.535238\n",
      "4  ministers   0.529630\n",
      "5  foreign     0.513192\n",
      "6  ministry    0.513114\n",
      "7  fund        0.507954\n",
      "8  investment  0.507153\n",
      "9  funds       0.504253\n",
      "Computing Similarity Using Model: Food\n",
      "                Word     Score\n",
      "0  already-stretched  0.342501\n",
      "1  11.27              0.337932\n",
      "2  time-sensitive     0.336961\n",
      "3  6-1                0.304646\n",
      "4  widowed            0.301399\n",
      "5  28lb.s             0.301364\n",
      "6  shichon            0.299284\n",
      "7  80-cup             0.296346\n",
      "8  jgibson            0.295999\n",
      "9  depleted           0.295393\n",
      "Computing Similarity Using Model: Bible\n",
      "Word not found in Bible - finance. Skipping...\n",
      "----------------******END*****----------------\n",
      "----------------******START*****----------------\n",
      "WORD: investment\n",
      "Computing Similarity Using Model: GoogleNews\n",
      "          Word     Score\n",
      "0  investments  0.809869\n",
      "1  investing    0.701224\n",
      "2  Investment   0.681215\n",
      "3  invesment    0.663090\n",
      "4  investor     0.631927\n",
      "5  invest       0.620340\n",
      "6  investors    0.594678\n",
      "7  equity       0.589751\n",
      "8  investement  0.561523\n",
      "9  Investments  0.559214\n",
      "Computing Similarity Using Model: GloVe\n",
      "          Word     Score\n",
      "0  investments  0.827808\n",
      "1  investing    0.657953\n",
      "2  fund         0.642288\n",
      "3  invest       0.633685\n",
      "4  firms        0.629304\n",
      "5  funds        0.619562\n",
      "6  asset        0.614389\n",
      "7  invested     0.611031\n",
      "8  investors    0.605211\n",
      "9  financial    0.589094\n",
      "Computing Similarity Using Model: Food\n",
      "                Word     Score\n",
      "0  cats'enjoyment.    0.435357\n",
      "1  bukhail            0.428857\n",
      "2  worth              0.375042\n",
      "3  20/month           0.369396\n",
      "4  worthwhile         0.364729\n",
      "5  gnawhide.          0.348910\n",
      "6  balls-             0.340807\n",
      "7  already-stretched  0.340073\n",
      "8  starbuck's..       0.337116\n",
      "9  treat/chew         0.333135\n",
      "Computing Similarity Using Model: Bible\n",
      "Word not found in Bible - investment. Skipping...\n",
      "----------------******END*****----------------\n",
      "----------------******START*****----------------\n",
      "WORD: acquire\n",
      "Computing Similarity Using Model: GoogleNews\n",
      "          Word     Score\n",
      "0  acquiring    0.708735\n",
      "1  aquire       0.692098\n",
      "2  acquired     0.678882\n",
      "3  acquires     0.619516\n",
      "4  acquisition  0.614354\n",
      "5  reacquire    0.608690\n",
      "6  purchase     0.601490\n",
      "7  Acquire      0.597112\n",
      "8  Acquiring    0.574335\n",
      "9  buy          0.573017\n",
      "Computing Similarity Using Model: GloVe\n",
      "          Word     Score\n",
      "0  acquiring    0.745367\n",
      "1  acquired     0.684805\n",
      "2  acquisition  0.662911\n",
      "3  buy          0.650142\n",
      "4  purchase     0.646091\n",
      "5  sell         0.611904\n",
      "6  stake        0.577788\n",
      "7  acquires     0.526760\n",
      "8  obtain       0.512650\n",
      "9  develop      0.501166\n",
      "Computing Similarity Using Model: Food\n",
      "            Word     Score\n",
      "0  acquired       0.356143\n",
      "1  grocey         0.355607\n",
      "2  drowsys        0.326805\n",
      "3  chinese-style  0.323381\n",
      "4  houten         0.314468\n",
      "5  roundness.     0.313839\n",
      "6  k.k.m.efendi   0.311978\n",
      "7  replaces.      0.308384\n",
      "8  wheat-y        0.299133\n",
      "9  chicha         0.294440\n",
      "Computing Similarity Using Model: Bible\n",
      "Word not found in Bible - acquire. Skipping...\n",
      "----------------******END*****----------------\n",
      "----------------******START*****----------------\n",
      "WORD: loss\n",
      "Computing Similarity Using Model: GoogleNews\n",
      "       Word     Score\n",
      "0  losses    0.711411\n",
      "1  losing    0.570874\n",
      "2  Loss      0.570495\n",
      "3  lost      0.539952\n",
      "4  setback   0.535180\n",
      "5  defeat    0.532587\n",
      "6  losss     0.491832\n",
      "7  Losses    0.489105\n",
      "8  drubbing  0.482614\n",
      "9  lossof    0.478108\n",
      "Computing Similarity Using Model: GloVe\n",
      "        Word     Score\n",
      "0  losses     0.706478\n",
      "1  losing     0.656665\n",
      "2  lost       0.636713\n",
      "3  result     0.595095\n",
      "4  lose       0.566108\n",
      "5  defeat     0.553116\n",
      "6  suffered   0.545165\n",
      "7  quarter    0.530605\n",
      "8  victory    0.509101\n",
      "9  suffering  0.503286\n",
      "Computing Similarity Using Model: Food\n",
      "                                Word     Score\n",
      "0  weight                             0.657302\n",
      "1  losing                             0.498762\n",
      "2  neurology                          0.488670\n",
      "3  reprogram                          0.486808\n",
      "4  *rash                              0.484226\n",
      "5  15-20+                             0.480702\n",
      "6  stasbilizes                        0.462115\n",
      "7  lose                               0.455461\n",
      "8  be-all-end-all                     0.451477\n",
      "9  =================================  0.451375\n",
      "Computing Similarity Using Model: Bible\n",
      "          Word     Score\n",
      "0  tooth        0.253132\n",
      "1  trouble      0.245818\n",
      "2  sad          0.244096\n",
      "3  asenath      0.239956\n",
      "4  birth-pains  0.226634\n",
      "5  pain         0.223487\n",
      "6  healthy      0.222996\n",
      "7  shocked      0.218537\n",
      "8  outlaw       0.218369\n",
      "9  peninnah     0.216078\n",
      "----------------******END*****----------------\n"
     ]
    }
   ],
   "source": [
    "find_similar('growth')\n",
    "find_similar('star')\n",
    "find_similar('finance')\n",
    "find_similar('investment')\n",
    "find_similar('acquire')\n",
    "find_similar('loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
